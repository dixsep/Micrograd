# üß† Micrograd

A minimalistic, educational implementation of a scalar-based automatic differentiation engine and a small neural network library built on top of it ‚Äî all in pure Python.

---

## ‚ú® Features

- **Autograd Engine**: Supports reverse-mode automatic differentiation.
- **Value Class**: Tracks operations and computes gradients.
- **Neural Network API**: Includes `Neuron`, `Layer`, and `MLP` abstractions.
- **Backpropagation**: Fully functional backward pass with graph traversal.
- **Educational**: Easy to understand and extend for learners.

---

## ‚öôÔ∏è Installation

Clone the repository:

```bash
git clone https://github.com/dixsep/Micrograd.git
cd Micrograd


---

## See the micrograd_demo.ipynb and trace_graph_demo.ipynb for usage.
